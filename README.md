Here's a professional and concise `README.md` you can use for your GitHub repository. It avoids any mention of ChatGPT and assumes that all result figures are saved in the `imgs/` folder.

---

# Dropout as a Bayesian Approximation: Reproduction

This repository reproduces the core results of the ICML 2016 paper:

> **Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning**
> *Yarin Gal & Zoubin Ghahramani, ICML 2016*

Our PyTorch implementation confirms that **MC-Dropout** provides a scalable, approximate Bayesian inference technique that yields calibrated epistemic uncertainty across a range of learning tasks.

## ğŸ“¦ Features

* âœ… UCI-10 regression benchmark
* âœ… Mauna Loa COâ‚‚ extrapolation
* âœ… Solar irradiance interpolation
* âœ… MNIST rotation uncertainty
* âœ… Thompson sampling in reinforcement learning

## ğŸ”§ Setup

Install the dependencies:

```bash
pip install -r requirements.txt
```

## ğŸš€ Running the Experiments

Each section has an independent script under `experiments/`. You can run them individually:

```bash
python experiments/uci_regression.py
python experiments/co2_forecasting.py
python experiments/solar_irradiance.py
python experiments/mnist_uncertainty.py
python experiments/rl_catch.py
```

## ğŸ–¼ï¸ Results

### UCI Regression

* Well-calibrated uncertainties across all 10 datasets.
* See: `imgs/uci_results_table.png`

### Mauna Loa COâ‚‚ Forecasting

<p align="center">
  <img src="imgs/1.1.png" width="45%">
  <img src="imgs/2.1.png" width="45%">
</p>

* ReLU: high variance beyond training range (epistemic uncertainty).
* Tanh: flatter extrapolation consistent with bounded GP prior.

### MNIST Digit Rotation

<p align="center">
  <img src="imgs/3.png" width="75%">
</p>

* Predictive entropy increases with rotation angle â€” uncertainty-aware behavior.

### Solar Irradiance Interpolation

<p align="center">
  <img src="imgs/solar_irradiance.png" width="80%">
</p>

* Tanh saturates with tight bounds; ReLU yields larger uncertainty in missing intervals.

### Reinforcement Learning: Catch Game

<p align="center">
  <img src="imgs/rl.png" width="60%">
</p>

* Thompson sampling with MC-Dropout achieves faster convergence vs. Îµ-greedy.

## ğŸ“„ Paper Reference

If you use this code or results, please cite:

> Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. *ICML 33*.

## ğŸ‘¤ Author

**Adnan Karim**
EURECOM
ğŸ“§ [adnan.karim@eurecom.fr](mailto:adnan.karim@eurecom.fr)

---

Let me know if you want a `requirements.txt` generated from your code.
